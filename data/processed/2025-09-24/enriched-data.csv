timestamp,acceptsLicense,installLocation,externalProvider,cloudProvider,providerDetails,primaryModel,otherModels,trainsModels,software,hasGpu,machineDetails,gpuDetails,rackDetails,hardwareDetails,installPurpose,performance,installCost,operationCost,costDetails,installReason,professionalUse,photo,website,name,additionalInfo,slug,seoTitle,seoDescription,h1Title,aiDescription
"2025/09/01 10:09:52 AM UTC+3","Oui","Directement sur ma machine","","","","Mistral","","Non, j'utilise des modèles existants","Open WebUI","Aucune - je n'utilise que le CPU","Dell latitude","Aucune","","","Juste pour moi","Lent (moins de 10 token/s) - le modèle doit tourner en toile de fond","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles","Non elle n'est pas vraiment fonctionnelle","https://drive.google.com/u/0/open?usp=forms_web&id=1DrddK_QqmWTOV5v4fqBg8Q_3mra_s7G6","https://www.linkedin.com/in/ericburel/","Eric Burel - LBKE","","eric-burel-lbke-2025-09-01-07-09-52-000","Installation LLM Mistral maison pour Eric Burel - LBKE","Utilisez Mistral sur votre Dell Latitude avec Open WebUI, sans coût ni matériel supplémentaire. Installation simple et open source pour une IA locale.","Déploiement Local de Mistral : Une IA Puissante Directement sur Votre Machine pa","Cette installation LLM est déployée directement sur une machine Dell Latitude, sans utilisation de GPU, fonctionnant uniquement sur CPU. Le système utilise Open WebUI comme interface logicielle et s'appuie sur le modèle Mistral, sans entraînement de modèles personnalisés. L'installation repose sur des outils open source, sans coût supplémentaire, et est conçue pour un usage personnel, avec une performance lente (moins de 10 tokens par seconde). L'objectif principal est l'expérimentation et la formation, notamment pour manipuler des données confidentielles.

Le résultat obtenu est une installation non optimisée, principalement utilisée à des fins éducatives et expérimentales. Bien que fonctionnelle, elle n'est pas adaptée à un usage professionnel en raison de ses limitations techniques, notamment la lenteur du traitement et l'absence de GPU. L'installation permet néanmoins de bidouiller et d'explorer les capacités des modèles LLM dans un environnement local et sécurisé."
"2025/09/10 2:53:55 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Mistral small 3.2","Non, j'utilise des modèles existants","ollama et openwebui les deux dockerisé","Oui sur machine personnelle","Dell Precision 5820","2x Nvidia Quadro pro 5000 soit 2x 16Go de vram donc 32Go de vram","","la machine vit a la cave, le cpu est un Intel Xeon W-2125 avec 2x 16Go de ram ddr4","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","la machine 800.- sur internet en occasion avec un quadro p5000 et la deuxième quadro p5000 120.- sur ebay l'année passée","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","https://drive.google.com/u/0/open?usp=forms_web&id=1wC-kK87GMqzH5mCJ3hWQmyIkG_ySuy3B;https://drive.google.com/u/0/open?usp=forms_web&id=1aggw_0MKV8c-nxMhi5p_XNoW612L395p","https://blog.trachsel.info/","Cédric Trachsel","les coût mois sont du a la consommation d'électricité la machine me coûte 9.-/mois, elle est arrêtée la nuit","cedric-trachsel-2025-09-10-11-53-55-000","Installation LLM gpt-oss-20b pour équipe et partage maison","Découvrez comment installer GPT-OSS-20B sur votre Dell Precision 5820 avec Ollama et OpenWebUI en Docker. Solution performante et économique, idéale pour une IA","Déployez GPT-OSS-20B en Local avec Cédric Trachsel pour une IA Puissante et Acce","Cette installation LLM est déployée sur une machine personnelle Dell Precision 5820 équipée d'un processeur Intel Xeon W-2125, de 32 Go de RAM DDR4 et de deux cartes graphiques Nvidia Quadro P5000, totalisant 32 Go de VRAM. Le système utilise deux logiciels Dockerisés : Ollama pour l'exécution des modèles et OpenWebUI pour l'interface utilisateur. Les modèles installés incluent GPT-OSS-20B et Mistral Small 3.2, sans entraînement local. La machine, située dans une cave, fonctionne avec une consommation électrique modérée, estimée à 9 € par mois, et est éteinte la nuit pour réduire les coûts.

Cette configuration permet une utilisation locale et autonome des modèles LLM, répondant à des besoins professionnels tout en garantissant la confidentialité des données. La performance est évaluée comme moyenne, avec un débit inférieur à 40 tokens par seconde, ce qui impose une attente pour les réponses. L'installation a été réalisée pour un coût total de 800 à 1000 €, incluant l'achat de la machine et des cartes graphiques d'occasion. Elle sert principalement à des fins d'expérimentation, de formation et de traitement de données sensibles, sans dépendre de fournisseurs externes ou de services cloud."
"2025/09/17 9:10:23 AM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Qwen","Non, j'utilise des modèles existants","LM Studio","Oui sur machine personnelle","Constuction personnel","NVIDIA 3090 (24 Go de VRAM)","","Machine indépendante sur réseau local avec 16Go de ram ddr4 ","Juste pour moi","Super rapide - peut traiter des entrées de grande taille très rapidement, potentiellement plus vite qu'avec une API","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","Les coûts mois sont dus à la consommation d'électricité. Elle est arrêtée la nuit.","Pour bidouiller et me former;Parce que je le peux;Pour utiliser des modèles débridés","Oui elle répond à des besoins professionnels concrets à mon échelle","","https://zonetuto.fr/","Christian Lemoussel","","christian-lemoussel-2025-09-17-06-10-23-000","Installation LLM gpt-oss-20b maison pour usage personnel et ","Découvrez comment installer GPT-OSS-20B sur votre PC avec LM Studio. Matériel personnalisé, coût maîtrisé (500-1000€). Puissance IA à portée de main !","Découvrez gpt-oss-20b : Un LLM Puissant Installé Localement par Christian Lemous","Cette installation LLM est déployée sur une machine personnelle équipée d'une carte graphique NVIDIA 3090 avec 24 Go de VRAM, 16 Go de RAM DDR4 et un stockage non spécifié. Le système fonctionne en local, sans dépendre d'un fournisseur cloud ou externe, et utilise LM Studio comme logiciel d'interface. L'installation inclut deux modèles : gpt-oss-20b comme modèle principal et Qwen comme modèle secondaire. La machine est conçue pour une utilisation autonome, avec une configuration matérielle dédiée et une connexion réseau locale. L'utilisateur a choisi de ne pas entraîner de modèles, préférant exploiter des versions pré-entraînées.

L'installation permet un traitement rapide des entrées de grande taille, avec des performances jugées supérieures à celles d'une API externe. Les coûts initiaux s'élèvent entre 500 et 1000 euros, correspondant principalement à l'achat du matériel, tandis que les dépenses mensuelles restent faibles, limitées à la consommation électrique. L'utilisateur exploite cette installation à la fois pour des besoins personnels et professionnels, notamment pour des tests et une utilisation débridée des modèles. La machine est éteinte la nuit pour optimiser les coûts énergétiques."
"2025/09/18 2:14:49 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","qwen:3b","","Non, j'utilise des modèles existants","Openweb UI","Aucune - je n'utilise que le CPU","Rizen 8845HS","","","16Go Ram","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","100-500€ - équivalent d'une carte graphique simple","0 - pas de maintenance particulière","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Pour des tâches mineures ou un usage qui reste personnel","","","","","2025-09-18-11-14-49-000","Installation LLM Qwen 3B : partage et communauté en local","Découvrez l'installation locale de Qwen-3B sur CPU Ryzen 8845HS avec OpenWeb UI, une solution économique (100-500€) pour une IA performante sans GPU.","Optimisation d'un LLM Qwen-3B sur Serveur Local : Performance et Autonomie","Cette installation repose sur un serveur local hébergé sur site ou chez un prestataire IT, équipé d'un processeur Rizen 8845HS et de 16 Go de RAM. Elle n'utilise pas de GPU, fonctionnant uniquement sur CPU. Le modèle principal déployé est Qwen-3B, avec une interface via Openweb UI. L'installation ne nécessite pas d'entraînement de modèles, se limitant à l'utilisation de modèles existants. La configuration, d'un coût initial de 100 à 500 euros, offre des performances rapides, dépassant 40 tokens par seconde, permettant des interactions en temps réel. Aucune maintenance spécifique n'est requise, et les coûts d'exploitation sont nuls.

L'installation est utilisée à des fins personnelles, pour des tâches mineures ou des expérimentations. Elle permet de garantir la souveraineté des données, évitant tout partage avec des tiers, y compris pour des informations non confidentielles. L'utilisateur privilégie cette solution pour des raisons de contrôle, de formation et d'autonomie technique, sans objectif professionnel. Le résultat obtenu est une solution autonome, performante et sécurisée, répondant aux besoins d'un usage individuel et confidentiel."
"2025/09/19 9:24:12 AM UTC+3","Oui","Directement sur ma machine","","","","mistral 7B","","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Dell Inspirons ","NVIDIA 4060 8Go","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données","Pour des tâches mineures ou un usage qui reste personnel","","","","Behel","2025-09-19-06-24-12-000","Installation LLM Mistral 7B pour usage personnel et partage ","Découvrez Mistral 7B installé localement sur votre Dell Inspiron avec Ollama, gratuitement et sans frais cachés. Une solution open source performante et accessi","Découvrez Mistral 7B : L'Intelligence Artificielle Puissante Directement sur Vot","Cette installation repose sur un modèle LLM Mistral 7B déployé localement via le logiciel Ollama, sur une machine personnelle équipée d'une carte graphique NVIDIA RTX 4060 avec 8 Go de mémoire. L'installation est réalisée directement sur un ordinateur Dell Inspiron, sans recours à un fournisseur externe ou à un service cloud. L'utilisateur a choisi de ne pas entraîner de modèles, se limitant à l'utilisation de modèles préexistants. La configuration inclut une licence acceptée et une absence de coûts supplémentaires, que ce soit pour l'installation ou l'exploitation, grâce à l'utilisation exclusive d'outils open source.

Le résultat obtenu est une solution performante, capable de générer des réponses en temps réel avec une vitesse supérieure à 40 tokens par seconde. L'installation répond aux besoins de l'utilisateur en matière de confidentialité et de souveraineté des données, évitant tout partage d'informations sensibles ou personnelles. Elle est utilisée à des fins personnelles ou pour des tâches mineures, sans maintenance particulière requise. L'absence de coûts opérationnels et d'infrastructure externe en fait une solution autonome et économique."
"2025/09/19 4:49:43 PM UTC+3","Oui","Directement sur ma machine","","","","mistral-small","mistral-nemo, gpt-oss, phi4","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Zotac Magnus (Mini PC Monté maison)","RTX4060Ti 16GoVRAM","","","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Pour garantir la souveraineté de mes données","Oui elle répond à des besoins professionnels concrets à mon échelle","","","","","2025-09-19-13-49-43-000","Installation LLM Mistral Small pour une utilisation communau","Découvrez Mistral Small installé sur un Zotac Magnus, un mini-PC puissant. Ollama simplifie l'utilisation, avec des coûts comparables à un MacBook haut de gamme","Découvrez Mistral-Small : Un Modèle LLM Puissant Directement sur Votre Machine","Cette installation repose sur un mini-PC personnalisé, un Zotac Magnus, équipé d'une carte graphique RTX 4060 Ti avec 16 Go de VRAM. Le système utilise le logiciel Ollama pour héberger des modèles de langage locaux, avec le modèle Mistral Small comme référence, complété par d'autres modèles comme Mistral Nemo, GPT-OSS et Phi4. L'installation est réalisée directement sur la machine personnelle, sans recours à un fournisseur externe ou à un service cloud. La configuration inclut une licence acceptée et une absence de formation de modèles, privilégiant l'utilisation de modèles pré-entraînés. La performance est évaluée comme moyenne, avec un débit inférieur à 40 tokens par seconde, nécessitant une attente pour les réponses. Le coût initial se situe entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, tandis que les coûts opérationnels sont nuls, sans maintenance spécifique.

Cette installation permet de répondre à des besoins professionnels concrets pour une petite équipe de 1 à 5 personnes, tout en garantissant la confidentialité et la souveraineté des données. Elle est utilisée pour des tâches de manipulation de données sensibles et comme outil d'apprentissage et d'expérimentation. Bien que la performance soit limitée par rapport à des solutions cloud, elle offre une autonomie complète et une maîtrise totale des données traitées. L'absence de frais récurrents et la simplicité de maintenance en font une solution adaptée à un usage professionnel localisé, malgré des performances modérées."
"2025/09/19 6:45:06 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss","llama, deepseekr1","Non, j'utilise des modèles existants","Ollama, Open-webui, continue","Oui sur machine personnelle","ASUS(mobo)+MSI(gpu)+AMD(cpu)","NVIDIA RTX 4090","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-19-15-45-06-000","Installation maison de GPT-OSS : LLM communautaire pour usag","Installez GPT-OSS sur votre PC avec ASUS, MSI et AMD pour une IA locale puissante. Utilisez Ollama, Open-webui et Continue. Investissement équivalent à un porta","Découvrez gpt-oss : L'IA puissante et open-source pour une utilisation locale su","Cette installation LLM est déployée localement sur une machine personnelle équipée d'une carte graphique NVIDIA RTX 4090, d'un processeur AMD et d'une carte mère ASUS. Le système utilise les logiciels Ollama et Open-webui pour héberger des modèles tels que gpt-oss, Llama et Deepseekr1, sans entraînement personnalisé. L'infrastructure repose sur un matériel haut de gamme, avec une configuration permettant des performances rapides, dépassant 40 tokens par seconde, ce qui offre un rendu temps réel. L'installation a nécessité un investissement initial compris entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, sans frais de maintenance ultérieurs.

L'installation est principalement utilisée à des fins personnelles et éducatives, avec une priorité accordée à la confidentialité des données. Elle permet de manipuler des informations sensibles sans recourir à des services externes, garantissant ainsi une souveraineté totale sur les données traitées. Bien que non optimisée pour un usage professionnel, elle répond aux besoins de l'utilisateur en matière d'expérimentation et de formation, tout en évitant toute dépendance à des infrastructures cloud ou à des fournisseurs tiers."
"2025/09/19 8:11:16 PM UTC+3","Oui","Sur un cloud (je ne gère pas le matériel ou je le loue)","","deepinfra","Hébergement de l'inférence via des entry point type openai","gpt-oss-120b","Qwen3-coder","Non, j'utilise des modèles existants","Openwebui, sst/opencode ","Aucune car je passe par un hébergement cloud","","","","","1-5 personnes","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","","","Charles Bonnissent ","","charles-bonnissent-2025-09-19-17-11-16-000","Installation LLM gpt-oss-120b en cloud pour Charles Bonnisse","Découvrez l'installation open source gpt-oss-120b sur cloud, sans coût matériel. Avec Openwebui et opencode, profitez d'une solution performante et accessible.","Découvrez gpt-oss-120b : L'IA puissante et accessible sur le cloud par Charles B","Cette installation repose sur une infrastructure cloud hébergée par DeepInfra, utilisant des endpoints compatibles avec l'API OpenAI. Le modèle principal déployé est gpt-oss-120b, complété par Qwen3-coder pour des tâches spécifiques. L'environnement logiciel inclut Openwebui et SST/opencode, sans gestion directe du matériel, car l'infrastructure est externalisée. L'absence de GPU dédié est compensée par une performance d'inférence rapide, dépassant 40 tokens par seconde, permettant un traitement en temps réel. L'installation ne nécessite aucun coût initial, les dépenses mensuelles restant limitées à 100 € maximum, couvrant uniquement l'hébergement cloud.

Cette configuration permet une utilisation à la fois personnelle et professionnelle, répondant aux besoins d'un petit groupe de 1 à 5 personnes. L'installation sert principalement à des fins d'expérimentation et de formation, tout en évitant la partage de données sensibles. Les réponses générées sont fluides et réactives, offrant une expérience utilisateur proche d'un système local, malgré l'hébergement distant. L'absence de coûts supplémentaires pour les modèles ou le matériel en fait une solution accessible pour des usages concrets à petite échelle."
"2025/09/21 12:42:12 AM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","llama2-uncensored","mistral, llama3","Non, j'utilise des modèles existants","ollama open webui","Oui sur un rack (installation sur site)","MS-01","RTX 3050 6Go","","70W","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","","Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour utiliser des modèles débridés","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-20-21-42-12-000","Installation LLM Llama2-uncensored : partage et communauté e","Déployez Llama2-uncensored sur un serveur local avec Ollama et Open WebUI. Matériel rackable MS-01, coûts maîtrisés (500-1000€). Autonomie et performance garant","Déploiement Local de Llama2-uncensored : Performance et Liberté sur Serveur Priv","Cette installation repose sur un serveur local hébergé sur site, équipé d'une carte graphique RTX 3050 avec 6 Go de mémoire vidéo. Le système utilise le logiciel Ollama Open WebUI pour déployer des modèles de langage pré-entraînés, notamment Llama2-uncensored, Mistral et Llama3. L'infrastructure est alimentée par une machine identifiée sous le nom MS-01, consommant 70W d'énergie. Bien que dotée d'une GPU, la performance reste limitée, avec un débit de tokens inférieur à 40 par seconde, ce qui impose des temps d'attente pour les réponses. L'installation a été réalisée pour un usage restreint, destiné à un petit groupe de 1 à 5 personnes, sans entraînement de modèles personnalisés.

L'installation permet d'utiliser des modèles de langage non censurés tout en évitant la transmission de données externes, répondant ainsi à des préoccupations de confidentialité. Cependant, son utilité professionnelle est limitée en raison de ses performances moyennes et de son manque de fonctionnalité optimale. Les coûts initiaux s'élèvent entre 500 et 1000 euros, équivalents à ceux d'un ordinateur portable ou d'une carte graphique gaming, tandis que les frais d'exploitation mensuels sont négligeables, inférieurs à 100 euros. L'objectif principal est de garantir un contrôle total sur les données traitées, au détriment de certaines performances et fonctionnalités avancées."
"2025/09/22 10:34:55 PM UTC+3","Oui","Directement sur ma machine","","","","Mistral 7B","Llama3.1, code-llama, gemma3n","Oui mais pour du machine learning/deep learning hors grands modèles","Ollama","Oui sur machine personnelle","Apple Macbook Pro M2 Max","","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données","Oui elle répond à des besoins professionnels concrets à mon échelle","","","","","2025-09-22-19-34-55-000","Installation LLM Mistral 7B pour usage personnel et partage ","Utilisez Mistral 7B gratuitement sur votre MacBook Pro M2 Max avec Ollama. Installation simple, open source, sans coût supplémentaire.","Découvrez Mistral 7B : L'Intelligence Artificielle Puissante Directement sur Vot","Cette installation repose sur un modèle LLM local, Mistral 7B, déployé via le logiciel Ollama sur un MacBook Pro M2 Max équipé d'une puce GPU. L'utilisateur a également intégré d'autres modèles comme Llama3.1, Code-Llama et Gemma3n, bien qu'il ne les entraîne pas pour des applications spécifiques aux grands modèles. L'infrastructure est entièrement locale, sans recours à un fournisseur cloud ou externe, et le matériel utilisé permet des performances rapides, dépassant 40 tokens par seconde, offrant ainsi une interaction en temps réel. L'installation est basée sur des outils open source, sans coût initial, et les dépenses opérationnelles restent limitées à moins de 100 € par mois.

L'installation répond à des besoins professionnels concrets tout en garantissant la confidentialité des données, un critère important pour l'utilisateur. Elle sert principalement à des expérimentations et à la formation, évitant ainsi la dépendance à des solutions externes. L'absence de partage de données personnelles ou d'entreprise, même non confidentielles, est un avantage notable, renforçant la souveraineté des informations traitées. L'utilisateur a opté pour cette configuration pour une utilisation personnelle et professionnelle, privilégiant la maîtrise des outils et des données."
"2025/09/22 10:46:07 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","En cours de selection","qwen3 / devstral /GML4.5/ Apertus / Lucie","Non, j'utilise des modèles existants","LmStudio/  Vllm / un dev perso","Mac Studio 512 Go ","Apple","512 Go","","","Juste pour moi","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","3000-10000€ - budget TPE/PME, rack de cartes graphiques","0 - pas de maintenance particulière","","Pour bidouiller et me former;Pour garantir la souveraineté de mes données;Propsoer des produits IA Offline","Oui elle répond à des besoins professionnels concrets à mon échelle","","https://www.ebii.fr","EBII s.a.s.u.","","ebii-s-a-s-u-2025-09-22-19-46-07-000","Installation LLM maison pour EBII : partage et communauté","Installation LLM locale sur Mac Studio 512 Go, optimisée avec LlamaStudio et vLLM. Solution flexible pour TPE/PME, hébergée sur site ou chez un prestataire IT. ","Optimisation d'un LLM sur Serveur Local : Solution Hébergée par EBII s.a.s.u.","Cette installation LLM est déployée sur un serveur hébergé localement, avec un Mac Studio équipé de 512 Go de mémoire. L'infrastructure repose sur des logiciels comme LMStudio et VLLM, complétés par un développement personnalisé. Aucun modèle n'est entraîné sur place, mais plusieurs options sont envisagées, dont Qwen3, Devstral, GML4.5, Apertus et Lucie. L'installation utilise une configuration matérielle dédiée, sans recourir à un fournisseur cloud ou externe. La performance est évaluée comme moyenne, avec un débit inférieur à 40 tokens par seconde, ce qui impose des temps d'attente pour les réponses. Le coût initial se situe entre 3 000 et 10 000 euros, correspondant à un budget TPE/PME, tandis que les coûts opérationnels sont nuls, sans maintenance spécifique.

Cette installation permet d'expérimenter et de se former à l'utilisation des LLM tout en garantissant la souveraineté des données. Elle répond à des besoins professionnels concrets, notamment pour proposer des produits IA en mode offline. L'objectif principal est de manipuler et d'explorer les capacités des modèles existants, sans visée de production à grande échelle. L'absence de maintenance régulière et les coûts opérationnels nuls en font une solution adaptée à un usage personnalisé et contrôlé."
"2025/09/23 9:05:22 AM UTC+3","Oui","Sur un cloud (je ne gère pas le matériel ou je le loue)","","runpod","","Qwen3-Coder-30B-A3B-Instruct","","Non, j'utilise des modèles existants","vLLM","Aucune car je passe par un hébergement cloud","","","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Car je ne suis pas satisfait des plateformes existantes et préfère mon installation locale;Pour utiliser des modèles débridés","","","","","","2025-09-23-06-05-22-000","Installation LLM Qwen3-Coder sur cloud pour usage personnel","Optimisez Qwen3-Coder-30B-A3B-Instruct sur cloud avec vLLM, sans coût matériel. Solution open source pour une IA performante et économique.","Découvrez Qwen3-Coder-30B-A3B-Instruct : Un LLM Puissant et Accessible sur le Cl","Cette installation repose sur une infrastructure cloud hébergée par RunPod, sans gestion directe du matériel. Elle utilise le modèle Qwen3-Coder-30B-A3B-Instruct comme principal modèle, déployé via le framework vLLM. L'installation ne nécessite pas de GPU dédié, car elle s'appuie sur les ressources cloud louées. Aucun modèle n'est entraîné localement, et l'ensemble des outils utilisés sont open source, sans coût supplémentaire pour l'installation. Les performances atteignent plus de 40 tokens par seconde, permettant des interactions en temps réel.

L'installation est utilisée à des fins personnelles, principalement pour l'expérimentation et la formation. Elle offre une alternative aux plateformes existantes, avec des modèles non restreints et une flexibilité accrue. Les coûts d'exploitation sont estimés entre 0 et 100 euros par mois, selon l'utilisation des ressources cloud. L'objectif principal est de disposer d'une solution locale et personnalisable, adaptée aux besoins spécifiques de l'utilisateur."
"2025/09/23 10:26:14 AM UTC+3","Oui","Directement sur ma machine","","","","","","Non, j'utilise des modèles existants","OpenWebUI","Oui sur machine personnelle","MacBook Air","M2 - 24 Go","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-23-07-26-14-000","Installation LLM maison pour usage personnel et partage comm","Installez un LLM open source sur votre MacBook Air avec OpenWebUI. Gratuit, rapide et sans frais cachés.","Optimisez votre expérience avec un LLM puissant installé directement sur votre m","Cette installation d'un modèle de langage local (LLM) est réalisée sur un MacBook Air équipé d'un processeur M2 et de 24 Go de mémoire graphique. L'utilisateur a opté pour une configuration autonome, sans recourir à un fournisseur externe ou cloud, en installant OpenWebUI directement sur sa machine. Le système exploite des modèles pré-entraînés sans formation supplémentaire, avec une performance évaluée à plus de 40 tokens par seconde, offrant une interaction en temps réel. L'ensemble repose sur des outils open source, sans coût d'acquisition ou de maintenance.

L'installation sert principalement à des fins personnelles, notamment pour l'expérimentation et l'apprentissage, tout en évitant la transmission de données sensibles. Bien que fonctionnelle, elle n'est pas utilisée dans un cadre professionnel. L'utilisateur privilégie cette solution pour son contrôle total sur les données et son absence de frais, malgré des limitations liées à l'utilisation d'un modèle non optimisé pour des applications critiques."