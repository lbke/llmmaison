timestamp,acceptsLicense,installLocation,externalProvider,cloudProvider,providerDetails,primaryModel,otherModels,trainsModels,software,hasGpu,machineDetails,gpuDetails,rackDetails,hardwareDetails,installPurpose,performance,installCost,operationCost,costDetails,installReason,professionalUse,photo,website,name,additionalInfo,slug,seoTitle,seoDescription,h1Title,aiDescription
"2025/09/01 10:09:52 AM UTC+3","Oui","Directement sur ma machine","","","","Mistral","","Non, j'utilise des modèles existants","Open WebUI","Aucune - je n'utilise que le CPU","Dell latitude","Aucune","","","Juste pour moi","Lent (moins de 10 token/s) - le modèle doit tourner en toile de fond","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles","Non elle n'est pas vraiment fonctionnelle","https://drive.google.com/u/0/open?usp=forms_web&id=1DrddK_QqmWTOV5v4fqBg8Q_3mra_s7G6","https://www.linkedin.com/in/ericburel/","Eric Burel - LBKE","","eric-burel-lbke-2025-09-01-07-09-52-000","Installation LLM Mistral maison pour Eric Burel - LBKE","Utilisez Mistral sur votre CPU avec Open WebUI - installation gratuite et open source pour une expérience LLM locale sans frais.","Déployez Mistral sur votre machine avec Eric Burel - LBKE pour une IA performant","Cette installation d'un modèle de langage (LLM) repose sur une configuration matérielle et logicielle minimaliste. Elle est déployée directement sur une machine Dell Latitude, sans GPU, en utilisant uniquement le CPU pour l'exécution. Le logiciel Open WebUI sert d'interface, tandis que le modèle principal utilisé est Mistral. L'installation n'inclut aucun modèle supplémentaire ni entraînement personnalisé, se limitant à l'utilisation de modèles préexistants. L'absence de GPU entraîne des performances lentes, avec un débit inférieur à 10 tokens par seconde, nécessitant une exécution en arrière-plan. L'ensemble du système repose sur des outils open source, sans coût d'installation ni de maintenance.

L'installation est principalement utilisée à des fins personnelles et éducatives, permettant à l'utilisateur de manipuler des données confidentielles sans recourir à des services externes. Bien que fonctionnelle, la configuration n'est pas optimisée pour un usage professionnel en raison de ses limitations techniques. L'objectif principal est l'expérimentation et l'apprentissage, avec une priorité donnée à la confidentialité des données. La solution reste basique, sans personnalisation avancée ni intégration avec des services cloud ou des fournisseurs externes."
"2025/09/10 2:53:55 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Mistral small 3.2","Non, j'utilise des modèles existants","ollama et openwebui les deux dockerisé","Oui sur machine personnelle","Dell Precision 5820","2x Nvidia Quadro pro 5000 soit 2x 16Go de vram donc 32Go de vram","","la machine vit a la cave, le cpu est un Intel Xeon W-2125 avec 2x 16Go de ram ddr4","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","la machine 800.- sur internet en occasion avec un quadro p5000 et la deuxième quadro p5000 120.- sur ebay l'année passée","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","https://drive.google.com/u/0/open?usp=forms_web&id=1wC-kK87GMqzH5mCJ3hWQmyIkG_ySuy3B;https://drive.google.com/u/0/open?usp=forms_web&id=1aggw_0MKV8c-nxMhi5p_XNoW612L395p","https://blog.trachsel.info/","Cédric Trachsel","les coût mois sont du a la consommation d'électricité la machine me coûte 9.-/mois, elle est arrêtée la nuit","cedric-trachsel-2025-09-10-11-53-55-000","Installation LLM GPT-OSS-20B pour partage communautaire","Découvrez comment installer GPT-OSS-20B sur votre Dell Precision 5820 avec Ollama et OpenWebUI en Docker. Solution performante et économique (500-1000€) pour un","Déployez gpt-oss-20b sur votre machine avec Cédric Trachsel pour une IA puissant","Cette installation repose sur une machine Dell Precision 5820 équipée d'un processeur Intel Xeon W-2125, de 32 Go de RAM DDR4 et de deux cartes graphiques Nvidia Quadro P5000, offrant un total de 32 Go de VRAM. Le système d'exploitation et les logiciels nécessaires, Ollama et OpenWebUI, sont déployés via Docker. L'installation utilise principalement le modèle GPT-OSS-20B, complété par Mistral Small 3.2, sans entraînement de modèles personnalisés. La machine est située dans une cave et fonctionne avec une consommation électrique modérée, estimée à 9 € par mois, en étant éteinte la nuit.

Cette configuration permet de répondre à des besoins professionnels concrets pour une petite équipe de 1 à 5 personnes, avec des performances moyennes, inférieures à 40 tokens par seconde. L'installation a été réalisée pour un coût total de 800 à 1000 €, incluant l'achat de la machine et des cartes graphiques d'occasion. Elle garantit la confidentialité des données, évitant tout partage avec des services externes, et offre une souveraineté complète sur les informations traitées."
"2025/09/17 9:10:23 AM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Qwen","Non, j'utilise des modèles existants","LM Studio","Oui sur machine personnelle","Constuction personnel","NVIDIA 3090 (24 Go de VRAM)","","Machine indépendante sur réseau local avec 16Go de ram ddr4 ","Juste pour moi","Super rapide - peut traiter des entrées de grande taille très rapidement, potentiellement plus vite qu'avec une API","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","Les coûts mois sont dus à la consommation d'électricité. Elle est arrêtée la nuit.","Pour bidouiller et me former;Parce que je le peux;Pour utiliser des modèles débridés","Oui elle répond à des besoins professionnels concrets à mon échelle","","https://zonetuto.fr/","Christian Lemoussel","","christian-lemoussel-2025-09-17-06-10-23-000","Installation LLM gpt-oss-20b maison pour usage personnel et ","Découvrez comment installer GPT-OSS-20B sur votre PC avec LM Studio. Matériel personnalisé, coût maîtrisé (500-1000€). Puissance IA locale à portée de main !","Découvrez gpt-oss-20b : Un LLM Puissant Installé Localement par Christian Lemous","Cette installation LLM est déployée sur une machine personnelle équipée d'une carte graphique NVIDIA 3090 avec 24 Go de VRAM, 16 Go de RAM DDR4 et un processeur non spécifié. Le système utilise le logiciel LM Studio pour héberger deux modèles : gpt-oss-20b comme modèle principal et Qwen comme modèle secondaire. L'installation est réalisée directement sur la machine, sans recours à un fournisseur cloud ou externe, et fonctionne en réseau local. La configuration est conçue pour une utilisation personnelle, avec une puissance suffisante pour traiter des entrées de grande taille rapidement, bien que l'arrêt nocturne limite son fonctionnement continu.

L'installation permet une utilisation à la fois personnelle et professionnelle, répondant à des besoins concrets à petite échelle. Les performances sont jugées rapides, potentiellement supérieures à celles d'une API commerciale, grâce à la puissance de calcul locale. Les coûts initiaux s'élèvent entre 500 et 1000 euros, correspondant principalement à l'achat de la carte graphique, tandis que les coûts opérationnels restent faibles, estimés entre 0 et 100 euros par mois, principalement liés à la consommation électrique. L'objectif principal est l'expérimentation et la formation, avec la possibilité d'utiliser des modèles sans restrictions."
"2025/09/18 2:14:49 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","qwen:3b","","Non, j'utilise des modèles existants","Openweb UI","Aucune - je n'utilise que le CPU","Rizen 8845HS","","","16Go Ram","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","100-500€ - équivalent d'une carte graphique simple","0 - pas de maintenance particulière","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Pour des tâches mineures ou un usage qui reste personnel","","","","","2025-09-18-11-14-49-000","Installation LLM Qwen-3B : partage et communauté en local","Découvrez l'installation locale de Qwen-3B sur CPU Ryzen 8845HS avec OpenWeb UI. Solution économique (100-500€) pour un LLM performant sans GPU, hébergée sur si","Optimisation d'un LLM Qwen-3B sur Serveur Local : Performance et Autonomie","Cette installation repose sur un serveur local hébergé sur site ou chez un prestataire IT, équipé d'un processeur Rizen 8845HS et de 16 Go de RAM. L'installation utilise uniquement le CPU, sans carte graphique, et fonctionne avec le modèle Qwen-3B via l'interface Openweb UI. L'utilisateur n'entraîne pas de modèles personnalisés et se limite aux versions préexistantes. Le coût initial, compris entre 100 et 500 euros, correspond à l'achat du matériel nécessaire, tandis que les coûts d'exploitation sont nuls, sans maintenance particulière requise.

L'installation permet d'obtenir des performances rapides, avec un débit supérieur à 40 tokens par seconde, offrant une sensation de temps réel dans les interactions. L'utilisateur l'emploie à des fins personnelles, notamment pour des tâches mineures ou pour se former, tout en garantissant la souveraineté et la confidentialité de ses données. L'absence de partage avec des fournisseurs externes ou des services cloud répond à une volonté de maîtrise totale de l'infrastructure."
"2025/09/19 9:24:12 AM UTC+3","Oui","Directement sur ma machine","","","","mistral 7B","","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Dell Inspirons ","NVIDIA 4060 8Go","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données","Pour des tâches mineures ou un usage qui reste personnel","","","","Behel","2025-09-19-06-24-12-000","Installation LLM Mistral 7B : Partage et utilisation maison","Découvrez Mistral 7B installé localement sur votre Dell Inspiron via Ollama, gratuitement et sans frais supplémentaires. Une solution open source performante et","Découvrez Mistral 7B : L'Intelligence Artificielle Puissante Directement sur Vot","Cette installation repose sur un modèle LLM Mistral 7B déployé localement via le logiciel Ollama, sans recours à un fournisseur externe ou cloud. L'installation est réalisée sur une machine personnelle Dell Inspiron équipée d'une carte graphique NVIDIA RTX 4060 avec 8 Go de mémoire vidéo, permettant une exécution rapide des modèles. L'utilisateur n'effectue pas d'entraînement de modèles, se limitant à l'utilisation de versions préexistantes. L'ensemble du système est configuré pour un usage individuel, sans maintenance particulière ni coût supplémentaire, les outils étant open source.

Le résultat obtenu est une solution performante, avec des vitesses de traitement supérieures à 40 tokens par seconde, offrant une interaction en temps réel. L'installation répond aux besoins de confidentialité et de souveraineté des données, évitant tout partage d'informations sensibles ou personnelles. Elle est utilisée pour des tâches mineures ou un usage personnel, sans implication professionnelle majeure. L'absence de coûts opérationnels et d'infrastructure cloud en fait une solution économique et autonome."
"2025/09/19 4:49:43 PM UTC+3","Oui","Directement sur ma machine","","","","mistral-small","mistral-nemo, gpt-oss, phi4","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Zotac Magnus (Mini PC Monté maison)","RTX4060Ti 16GoVRAM","","","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Pour garantir la souveraineté de mes données","Oui elle répond à des besoins professionnels concrets à mon échelle","","","","","2025-09-19-13-49-43-000","Installation LLM Mistral Small : Partage communautaire et us","Découvrez Mistral-small installé sur un Zotac Magnus, un mini-PC puissant et personnalisé. Ollama facilite l'utilisation, pour une expérience LLM locale et perf","Découvrez Mistral-Small : LLM Puissant et Léger pour une IA Locale Optimale","Cette installation repose sur un mini-PC personnalisé, un Zotac Magnus, équipé d'une carte graphique RTX 4060 Ti avec 16 Go de VRAM. Le système utilise le logiciel Ollama pour héberger des modèles de langage locaux, avec le modèle Mistral Small comme référence, complété par d'autres options comme Mistral Nemo, GPT-OSS et Phi4. L'installation est réalisée directement sur la machine personnelle, sans recours à un fournisseur externe ou cloud. La configuration matérielle permet une exécution locale des modèles, avec une performance évaluée comme moyenne, générant moins de 40 tokens par seconde. Le coût initial se situe entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, tandis que les coûts opérationnels sont nuls, sans maintenance spécifique requise.

Cette installation répond à des besoins professionnels et personnels, notamment pour manipuler des données confidentielles et garantir leur souveraineté. Elle est utilisée par un petit groupe de 1 à 5 personnes, principalement pour des tâches de formation et d'expérimentation. La performance, bien que limitée, permet une utilisation interactive, avec des temps de réponse acceptables pour des échanges simples ou des traitements en arrière-plan. L'absence de frais récurrents et la maîtrise totale des données en font une solution adaptée à des usages sensibles ou autonomes."
"2025/09/19 6:45:06 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss","llama, deepseekr1","Non, j'utilise des modèles existants","Ollama, Open-webui, continue","Oui sur machine personnelle","ASUS(mobo)+MSI(gpu)+AMD(cpu)","NVIDIA RTX 4090","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-19-15-45-06-000","Installation maison de GPT-OSS : LLM communautaire pour usag","Installez GPT-OSS sur votre PC (ASUS/AMD/MSI) avec Ollama et Open-webui. Performances locales, coût maîtrisé (1000-3000€).","Découvrez gpt-oss : L'IA puissante directement sur votre machine pour des répons","Cette installation d'un modèle de langage local (LLM) repose sur une configuration matérielle dédiée, incluant une carte graphique NVIDIA RTX 4090, un processeur AMD et une carte mère ASUS. Le système utilise le logiciel Ollama pour l'exécution des modèles, avec une interface Open-webui et l'outil Continue pour l'interaction. Les modèles déployés sont principalement gpt-oss, accompagné de Llama et Deepseekr1, tous pré-entraînés sans formation supplémentaire. L'installation est hébergée directement sur une machine personnelle équipée d'un GPU, ce qui permet des performances rapides, dépassant 40 tokens par seconde, avec un temps de réponse perçu comme temps réel.

L'installation est principalement utilisée à des fins personnelles et éducatives, avec une priorité accordée à la confidentialité des données. Elle permet de manipuler des informations sensibles sans recourir à des services externes, garantissant ainsi une souveraineté totale sur les données traitées. Bien que non optimisée pour un usage professionnel, elle offre une solution autonome et économique, avec des coûts initiaux estimés entre 1000 et 3000 euros, sans frais de maintenance ultérieurs. L'objectif principal est d'expérimenter et de se former, tout en évitant la dépendance à des infrastructures cloud."
"2025/09/19 8:11:16 PM UTC+3","Oui","Sur un cloud (je ne gère pas le matériel ou je le loue)","","deepinfra","Hébergement de l'inférence via des entry point type openai","gpt-oss-120b","Qwen3-coder","Non, j'utilise des modèles existants","Openwebui, sst/opencode ","Aucune car je passe par un hébergement cloud","","","","","1-5 personnes","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","","","Charles Bonnissent ","","charles-bonnissent-2025-09-19-17-11-16-000","Installation LLM GPT-OSS-120B en cloud pour 5 utilisateurs","Découvrez une installation LLM open source avec gpt-oss-120b sur cloud, sans coût matériel. Utilisez Openwebui et OpenCode pour une expérience optimisée.","Déploiement Cloud du Modèle GPT-OSS-120B par Charles Bonnissent","Cette installation repose sur une infrastructure cloud hébergée par DeepInfra, utilisant des endpoints compatibles avec l'API OpenAI. Le modèle principal déployé est gpt-oss-120b, complété par Qwen3-coder pour des tâches spécifiques. L'environnement logiciel combine Openwebui et sst/opencode, sans gestion directe du matériel, ce qui exclut l'utilisation de GPU dédiés. L'infrastructure est conçue pour un usage limité à cinq personnes, avec une performance évaluée à plus de 40 tokens par seconde, offrant un temps de réponse perçu comme instantané. Les coûts d'installation sont nuls, tandis que les frais d'exploitation varient entre 0 et 100 euros par mois, selon l'utilisation.

L'installation répond à des besoins professionnels tout en respectant des contraintes de confidentialité, évitant le partage de données personnelles ou d'entreprise. Elle est principalement utilisée pour des expérimentations et une formation personnelle, tout en couvrant des cas d'usage concrets à petite échelle. L'absence de formation de modèles sur mesure limite l'installation à l'exploitation de modèles préexistants, mais garantit une mise en œuvre simple et économique."
"2025/09/21 12:42:12 AM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","llama2-uncensored","mistral, llama3","Non, j'utilise des modèles existants","ollama open webui","Oui sur un rack (installation sur site)","MS-01","RTX 3050 6Go","","70W","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","","Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour utiliser des modèles débridés","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-20-21-42-12-000","Installation LLM Llama2-uncensored : partage et communauté e","Déployez un LLM puissant (Llama2-uncensored) sur site avec Ollama et Open WebUI. Matériel rackable MS-01, installation locale. Budget : 500-1000€. Autonomie et ","Déploiement Local de Llama2-uncensored : Performance et Confidentialité sur Serv","Cette installation LLM est déployée sur un serveur local hébergé sur site, équipé d'une carte graphique RTX 3050 avec 6 Go de mémoire. Le matériel, identifié sous le nom de code MS-01, consomme environ 70W et est installé dans un rack. L'infrastructure utilise le logiciel Ollama Open WebUI pour héberger les modèles, avec Llama2-uncensored comme modèle principal, complété par Mistral et Llama3. L'installation ne dispose pas de capacités de formation de modèles, se limitant à l'utilisation de modèles pré-entraînés. La performance est évaluée comme moyenne, avec un débit inférieur à 40 tokens par seconde, nécessitant une attente pour les réponses. Le coût initial s'élève entre 500 et 1000 euros, tandis que les coûts opérationnels mensuels restent inférieurs à 100 euros.

L'installation répond à des besoins spécifiques de confidentialité, évitant le partage de données personnelles ou professionnelles, même non sensibles. Elle permet l'utilisation de modèles non censurés, bien que son usage professionnel soit limité en raison de sa performance moyenne. Destinée à un usage restreint (1 à 5 personnes), elle offre une solution autonome mais avec des contraintes en termes de rapidité et de fonctionnalité. L'objectif principal est de garantir un contrôle total sur les données, au détriment de certaines performances et de la scalabilité."
"2025/09/22 10:34:55 PM UTC+3","Oui","Directement sur ma machine","","","","Mistral 7B","Llama3.1, code-llama, gemma3n","Oui mais pour du machine learning/deep learning hors grands modèles","Ollama","Oui sur machine personnelle","Apple Macbook Pro M2 Max","","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données","Oui elle répond à des besoins professionnels concrets à mon échelle","","","","","2025-09-22-19-34-55-000","Installation LLM Mistral 7B : Partage et utilisation maison","Utilisez Mistral 7B gratuitement sur votre MacBook Pro M2 Max avec Ollama. Installation simple, open source, sans coût supplémentaire.","Découvrez Mistral 7B : L'Intelligence Artificielle Puissante Directement sur Vot","Cette installation d'un modèle de langage local repose sur un MacBook Pro M2 Max équipé d'une carte graphique, permettant une exécution performante avec des vitesses de traitement dépassant 40 tokens par seconde. Le système utilise le logiciel Ollama pour héberger le modèle principal Mistral 7B, complété par d'autres modèles comme Llama3.1, Code-Llama et Gemma3n. L'utilisateur a également la capacité d'entraîner des modèles, bien que cette fonctionnalité soit limitée aux applications de machine learning et deep learning hors grands modèles. L'installation est entièrement locale, sans recours à un fournisseur externe ou cloud, et repose sur des outils open source, ce qui élimine tout coût initial.

L'installation répond à des besoins professionnels concrets tout en garantissant une souveraineté totale des données, évitant ainsi tout partage d'informations personnelles ou d'entreprise. Elle sert principalement à des fins d'expérimentation et de formation, avec une performance perçue comme temps réel grâce à la rapidité de traitement. Les coûts d'exploitation sont minimes, estimés entre 0 et 100 euros par mois, principalement liés à la consommation énergétique. L'utilisateur a opté pour cette solution pour conserver un contrôle total sur ses données et éviter toute dépendance à des infrastructures externes."
"2025/09/22 10:46:07 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","En cours de selection","qwen3 / devstral /GML4.5/ Apertus / Lucie","Non, j'utilise des modèles existants","LmStudio/  Vllm / un dev perso","Mac Studio 512 Go ","Apple","512 Go","","","Juste pour moi","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","3000-10000€ - budget TPE/PME, rack de cartes graphiques","0 - pas de maintenance particulière","","Pour bidouiller et me former;Pour garantir la souveraineté de mes données;Propsoer des produits IA Offline","Oui elle répond à des besoins professionnels concrets à mon échelle","","https://www.ebii.fr","EBII s.a.s.u.","","ebii-s-a-s-u-2025-09-22-19-46-07-000","Installation LLM maison pour EBII : partage et communauté lo","Installation LLM locale sur Mac Studio 512 Go avec LmStudio, Vllm et outils personnalisés. Solution flexible pour TPE/PME, hébergée sur site ou chez un prestata","Déploiement Local d'un LLM sur Serveur Hébergé par EBII s.a.s.u.","Cette installation LLM est déployée sur un serveur hébergé localement, avec un Mac Studio équipé de 512 Go de mémoire. L'infrastructure utilise des logiciels tels que LMStudio et VLLM, ainsi qu'un développement personnalisé. Aucun modèle n'est entraîné sur place, mais plusieurs options sont envisagées, notamment Qwen3, Devstral, GML4.5, Apertus et Lucie. L'installation repose sur une configuration matérielle dédiée, avec un budget compris entre 3 000 et 10 000 euros, couvrant l'achat et la maintenance du matériel. Les performances sont estimées comme moyennes, avec un débit inférieur à 40 tokens par seconde, ce qui implique des temps de réponse plus longs. L'objectif principal est d'assurer la souveraineté des données et de proposer des solutions IA hors ligne, tout en servant des besoins professionnels concrets.

Cette installation permet d'expérimenter et de se former aux technologies LLM tout en garantissant un contrôle total sur les données traitées. Elle est utilisée à la fois pour des projets personnels et professionnels, notamment pour développer des produits IA autonomes. Bien que les coûts d'exploitation soient nuls, l'investissement initial reste significatif, justifié par la nécessité d'une infrastructure locale et sécurisée. Les résultats obtenus répondent aux attentes en termes de souveraineté et de fonctionnalités, malgré des performances limitées par le matériel disponible. L'installation s'inscrit dans une démarche de maîtrise technique et d'indépendance vis-à-vis des solutions cloud."
"2025/09/23 9:05:22 AM UTC+3","Oui","Sur un cloud (je ne gère pas le matériel ou je le loue)","","runpod","","Qwen3-Coder-30B-A3B-Instruct","","Non, j'utilise des modèles existants","vLLM","Aucune car je passe par un hébergement cloud","","","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Car je ne suis pas satisfait des plateformes existantes et préfère mon installation locale;Pour utiliser des modèles débridés","","","","","","2025-09-23-06-05-22-000","Installation LLM Qwen3-Coder pour usage personnel en cloud","Découvrez Qwen3-Coder-30B-A3B-Instruct sur cloud avec vLLM : installation open source, zéro coût, sans gestion matérielle.","Découvrez Qwen3-Coder-30B-A3B-Instruct : Un LLM Puissant et Accessible sur le Cl","Cette installation repose sur une infrastructure cloud hébergée par RunPod, sans gestion directe du matériel. Elle utilise le modèle Qwen3-Coder-30B-A3B-Instruct comme principal modèle de langage, déployé via le framework vLLM. L'installation ne comporte pas de GPU dédié, le traitement étant entièrement externalisé. Aucun modèle n'est entraîné localement, l'utilisateur se limitant à l'exploitation de modèles préexistants. L'ensemble est configuré pour un usage personnel, avec une performance évaluée à plus de 40 tokens par seconde, permettant des interactions en temps réel. Les coûts d'exploitation sont estimés entre 0 et 100 euros par mois, selon l'utilisation.

Le résultat obtenu est une plateforme personnalisée, offrant une alternative aux solutions commerciales existantes. L'utilisateur bénéficie d'une flexibilité accrue, notamment pour expérimenter avec des modèles débridés et adapter l'outil à ses besoins spécifiques. L'absence de frais supplémentaires pour l'installation, combinée à des coûts opérationnels maîtrisés, en fait une solution économique pour un usage individuel ou éducatif. La rapidité des réponses et l'absence de restrictions imposées par des plateformes tierces constituent les principaux avantages de cette configuration."
"2025/09/23 10:26:14 AM UTC+3","Oui","Directement sur ma machine","","","","","","Non, j'utilise des modèles existants","OpenWebUI","Oui sur machine personnelle","MacBook Air","M2 - 24 Go","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-23-07-26-14-000","Installation LLM maison pour usage personnel et partage comm","Installez un LLM sur votre MacBook Air avec OpenWebUI, gratuitement et sans frais cachés. Solution open source pour une IA locale et performante.","Découvrez un LLM Puissant Installé Directement sur Votre Machine","Cette installation d'un LLM (Large Language Model) est réalisée directement sur une machine personnelle, un MacBook Air équipé d'un processeur M2 et de 24 Go de mémoire graphique. L'utilisateur a opté pour OpenWebUI comme interface logicielle, sans recourir à un fournisseur externe ou à un service cloud. Le système exploite des modèles pré-entraînés, sans formation ou personnalisation supplémentaire. La configuration, entièrement basée sur des outils open source, ne génère aucun coût d'installation ou de fonctionnement. L'équipement dispose d'une carte graphique intégrée, ce qui permet des performances rapides, avec un débit de plus de 40 tokens par seconde, offrant une interaction en temps réel.

L'installation est principalement utilisée à des fins personnelles, notamment pour l'expérimentation et l'apprentissage. L'utilisateur privilégie cette solution pour éviter de partager des données, qu'elles soient personnelles ou professionnelles, même non sensibles. Bien que fonctionnelle, l'installation n'est pas destinée à un usage professionnel, son objectif étant avant tout l'exploration technique et l'autonomie. Aucun coût supplémentaire n'est engagé, que ce soit pour l'achat de matériel ou la maintenance, ce qui en fait une solution économique et autonome."
"2025/09/24 1:46:33 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss:20b ","llama3.2-vision","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Apple","Apple Metal","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Pour garantir la souveraineté de mes données","Elle est au coeur du fonctionnement de mon organisation/de mon travail quotidien","","","CNRS","","cnrs-2025-09-24-10-46-33-000","Installation LLM GPT-OSS 20B pour usage personnel au CNRS","Découvrez l'installation de GPT-OSS 20B sur votre Mac avec Ollama : une IA puissante, locale et économique, pour un coût équivalent à un portable haut de gamme.","Découvrez le modèle GPT-OSS 20B du CNRS pour une IA puissante directement sur vo","Cette installation LLM est déployée localement sur une machine personnelle équipée d'un GPU Apple Metal, utilisant le framework Ollama. Elle intègre deux modèles principaux : gpt-oss:20b et llama3.2-vision, sans entraînement de modèles personnalisés. L'infrastructure repose sur un ordinateur portable haut de gamme, d'une valeur estimée entre 1000 et 3000 euros, offrant des performances rapides (plus de 40 tokens par seconde), permettant des interactions en temps réel. L'installation est conçue pour une utilisation exclusive, sans dépendance à un fournisseur externe ou à un service cloud, garantissant ainsi une souveraineté totale des données.

L'installation est utilisée à des fins professionnelles au sein du CNRS, où elle joue un rôle central dans le traitement de données confidentielles. Son coût opérationnel est nul, sans maintenance particulière requise. Elle répond à des besoins spécifiques liés à la manipulation de données sensibles et à la formation technique, tout en évitant les risques liés aux solutions hébergées. L'objectif principal est de permettre une utilisation flexible et sécurisée des modèles de langage, en alignement avec les exigences de confidentialité et d'autonomie de l'organisation."
"2025/09/27 2:32:42 AM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","dinum en plus de mon installe localz","","","llama3.1","","Non, j'utilise des modèles existants","Python maison pour optimiser mon rag","un mac studio dedier","mac","Mac studio 64Go de ram","","","20-50 personnes","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Car je manipule des données confidentielles;RAG des procedures du labo","Oui elle répond à des besoins professionnels concrets à mon échelle","","","YANN DELCAMBRE LATMOS","","yann-delcambre-latmos-2025-09-26-23-32-42-000","Installation LLM Llama3.1 pour 20-50 utilisateurs chez YANN ","Déployez Llama3.1 sur un Mac Studio dédié, optimisé par un RAG maison. Solution locale, performante et économique (1000-3000€), idéale pour une IA sur mesure.","Llama3.1 : Installation sur Serveur Local par Yann Delcambre - Performances Opti","Cette installation repose sur un serveur local hébergé sur un Mac Studio équipé de 64 Go de RAM, dédié à l'exécution de modèles de langage. Le système utilise le modèle Llama 3.1 comme base principale, sans entraînement de modèles personnalisés. L'infrastructure est optimisée par un logiciel Python maison pour le Retrieval-Augmented Generation (RAG), permettant d'intégrer des procédures internes. L'installation ne nécessite pas de GPU dédié, bien que le Mac Studio offre des performances suffisantes pour un traitement rapide, avec une vitesse de génération supérieure à 40 tokens par seconde. Le coût initial se situe entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, sans frais de maintenance supplémentaires.

L'installation répond aux besoins professionnels d'une équipe de 20 à 50 personnes, en traitant des données confidentielles liées aux procédures de laboratoire. Les réponses générées offrent une expérience temps réel, grâce à la rapidité du système. L'absence de coûts opérationnels et la simplicité de maintenance en font une solution adaptée à un usage interne, tout en garantissant la sécurité des informations sensibles. L'installation est utilisée pour des applications concrètes, notamment l'accès et l'analyse de documents internes via le RAG."
"2025/09/30 10:25:02 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","gemma3:270m","deepseek-r1:1.5b
hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:latest
qwen3:4b","Non, j'utilise des modèles existants","OpenWebUI Ollama","Aucune - je n'utilise que le CPU","TuringPi 2 / Modules Turing RK1 16 et 8 Go","N/A","N/A","","Juste pour moi","Lent (moins de 10 token/s) - le modèle doit tourner en toile de fond","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former","Pour des tâches mineures ou un usage qui reste personnel","","","Patrice Cosson","","patrice-cosson-2025-09-30-19-25-02-000","Installation LLM Gemma3:270m - Serveur local pour usage pers","Découvrez Gemma3:270m sur serveur local avec TuringPi 2, CPU-only, OpenWebUI Ollama. Solution open source sans coût supplémentaire, idéale pour une IA performan","Découvrez l'installation locale de Gemma3:270m par Patrice Cosson - Performances","Cette installation LLM est déployée sur un serveur local hébergé par un prestataire IT ou sur site, utilisant un matériel payé par l'utilisateur. L'infrastructure repose sur un TuringPi 2 équipé de modules Turing RK1 avec 16 Go et 8 Go de RAM, fonctionnant sans GPU. Le système exploite des modèles open source, dont gemma3:270m comme modèle principal, ainsi que deepseek-r1:1.5b, Qwen3-4B-Instruct et qwen3:4b. L'installation utilise OpenWebUI Ollama pour l'interface et ne nécessite aucune formation de modèles, se limitant à l'utilisation de versions pré-entraînées. L'absence de GPU entraîne des performances lentes, avec un débit inférieur à 10 tokens par seconde, obligeant les modèles à fonctionner en arrière-plan.

L'installation est principalement utilisée à des fins personnelles, sans objectif professionnel. Elle sert d'outil d'apprentissage et d'expérimentation, avec un coût d'installation et de fonctionnement nul, reposant uniquement sur des logiciels open source. Les performances limitées en font un système adapté à des tâches mineures ou à un usage occasionnel, sans exigence de réactivité élevée. L'utilisateur, Patrice Cosson, n'a pas partagé de détails supplémentaires sur son utilisation ou ses résultats."