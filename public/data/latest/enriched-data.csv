timestamp,acceptsLicense,installLocation,externalProvider,cloudProvider,providerDetails,primaryModel,otherModels,trainsModels,software,hasGpu,machineDetails,gpuDetails,rackDetails,hardwareDetails,installPurpose,performance,installCost,operationCost,costDetails,installReason,professionalUse,photo,website,name,additionalInfo,slug,seoTitle,seoDescription,h1Title,aiDescription
"2025/09/01 10:09:52 AM UTC+3","Oui","Directement sur ma machine","","","","Mistral","","Non, j'utilise des modèles existants","Open WebUI","Aucune - je n'utilise que le CPU","Dell latitude","Aucune","","","Juste pour moi","Lent (moins de 10 token/s) - le modèle doit tourner en toile de fond","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles","Non elle n'est pas vraiment fonctionnelle","https://drive.google.com/u/0/open?usp=forms_web&id=1DrddK_QqmWTOV5v4fqBg8Q_3mra_s7G6","https://www.linkedin.com/in/ericburel/","Eric Burel - LBKE","","eric-burel-lbke-2025-09-01-07-09-52-000","Installation LLM Mistral maison pour Eric Burel - LBKE","Utilisez Mistral directement sur votre PC avec Open WebUI, sans coût ni matériel supplémentaire. Solution open source optimisée pour CPU.","Déploiement Local de Mistral : Optimisez Votre IA avec Eric Burel et LBKE","Cette installation LLM est configurée sur une machine Dell Latitude, sans GPU, utilisant uniquement le CPU pour le traitement. L'installation est réalisée directement sur l'ordinateur personnel de l'utilisateur, sans recours à un fournisseur externe ou à un service cloud. Le modèle principal utilisé est Mistral, tandis que l'utilisateur n'entraîne pas de modèles personnalisés, se limitant à des modèles existants. L'interface logicielle employée est Open WebUI, et l'installation est destinée à un usage personnel, sans finalité professionnelle.

Les performances de cette installation sont lentes, avec un débit inférieur à 10 tokens par seconde, ce qui nécessite que le modèle fonctionne en arrière-plan. L'installation n'a engendré aucun coût supplémentaire, reposant uniquement sur des outils open source. L'utilisateur n'a pas mentionné de frais de maintenance ou d'exploitation. L'objectif principal de cette installation est l'expérimentation et la formation, ainsi que la manipulation de données confidentielles, ce qui justifie le choix d'une solution locale.

L'installation n'est pas considérée comme pleinement fonctionnelle pour un usage professionnel, mais elle répond aux besoins de l'utilisateur en matière d'apprentissage et de manipulation de données sensibles. Aucune information supplémentaire n'est fournie concernant les détails techniques ou les configurations spécifiques. L'utilisateur a partagé une photo de l'installation et un lien vers son profil LinkedIn pour référence."
"2025/09/10 2:53:55 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Mistral small 3.2","Non, j'utilise des modèles existants","ollama et openwebui les deux dockerisé","Oui sur machine personnelle","Dell Precision 5820","2x Nvidia Quadro pro 5000 soit 2x 16Go de vram donc 32Go de vram","","la machine vit a la cave, le cpu est un Intel Xeon W-2125 avec 2x 16Go de ram ddr4","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","la machine 800.- sur internet en occasion avec un quadro p5000 et la deuxième quadro p5000 120.- sur ebay l'année passée","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","https://drive.google.com/u/0/open?usp=forms_web&id=1wC-kK87GMqzH5mCJ3hWQmyIkG_ySuy3B;https://drive.google.com/u/0/open?usp=forms_web&id=1aggw_0MKV8c-nxMhi5p_XNoW612L395p","https://blog.trachsel.info/","Cédric Trachsel","les coût mois sont du a la consommation d'électricité la machine me coûte 9.-/mois, elle est arrêtée la nuit","cedric-trachsel-2025-09-10-11-53-55-000","Installation LLM GPT-OSS-20B pour partage communautaire","Découvrez comment installer GPT-OSS-20B sur votre Dell Precision 5820 avec Ollama et OpenWebUI en Docker. Solution performante et économique pour une IA locale.","Découvrez gpt-oss-20b : Un LLM Puissant et Autonome sur Votre Machine par Cédric","Cette installation LLM est déployée localement sur une machine personnelle, un Dell Precision 5820 équipé d'un processeur Intel Xeon W-2125 et de 32 Go de RAM DDR4. Le système utilise deux cartes graphiques Nvidia Quadro P5000, chacune dotée de 16 Go de VRAM, pour accélérer les calculs. L'installation repose sur deux logiciels Dockerisés : Ollama et OpenWebUI, permettant d'exploiter les modèles GPT-OSS-20B et Mistral Small 3.2 sans formation personnalisée. La machine, située dans une cave, est utilisée pour des besoins professionnels et personnels, avec une performance modérée de moins de 40 tokens par seconde.

L'installation a été réalisée pour des raisons de confidentialité et de souveraineté des données, évitant ainsi le recours à des services cloud externes. Le coût total s'élève à environ 800 euros pour la machine et 120 euros pour la seconde carte graphique, achetées d'occasion. Les dépenses mensuelles se limitent à environ 9 euros pour la consommation électrique, la machine étant éteinte la nuit. L'installation répond à des besoins concrets pour une petite équipe de 1 à 5 personnes, avec une utilisation principalement axée sur l'expérimentation et la formation.

L'installation est documentée sur le site personnel de l'utilisateur, Cédric Trachsel, et inclut des photos de la configuration. Le choix des modèles et des outils reflète une approche pragmatique, privilégiant la flexibilité et le contrôle local sur les données. Bien que la performance soit limitée, elle reste adaptée à des usages professionnels spécifiques, notamment pour le traitement de données sensibles."
"2025/09/17 9:10:23 AM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss-20b","Qwen","Non, j'utilise des modèles existants","LM Studio","Oui sur machine personnelle","Constuction personnel","NVIDIA 3090 (24 Go de VRAM)","","Machine indépendante sur réseau local avec 16Go de ram ddr4 ","Juste pour moi","Super rapide - peut traiter des entrées de grande taille très rapidement, potentiellement plus vite qu'avec une API","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","Les coûts mois sont dus à la consommation d'électricité. Elle est arrêtée la nuit.","Pour bidouiller et me former;Parce que je le peux;Pour utiliser des modèles débridés","Oui elle répond à des besoins professionnels concrets à mon échelle","","https://zonetuto.fr/","Christian Lemoussel","","christian-lemoussel-2025-09-17-06-10-23-000","Installation LLM gpt-oss-20b maison pour Christian Lemoussel","Découvrez l'installation du modèle GPT-OSS-20B sur votre machine avec LM Studio. Matériel personnalisé, coûts maîtrisés (500-1000€). Puissance IA à portée de ma","Découvrez gpt-oss-20b : Un LLM Puissant Installé Localement par Christian Lemous","Cette installation LLM est configurée sur une machine personnelle dédiée, installée directement sur l'équipement de l'utilisateur. Elle utilise le modèle principal gpt-oss-20b, avec Qwen comme modèle secondaire. L'installation repose sur le logiciel LM Studio et s'exécute sur une carte graphique NVIDIA 3090 dotée de 24 Go de VRAM, ce qui permet des performances élevées pour le traitement de grandes entrées. La machine dispose de 16 Go de RAM DDR4 et fonctionne en tant qu'unité indépendante sur un réseau local, sans dépendre de fournisseurs externes ou de services cloud.

L'installation a été réalisée pour des usages personnels et professionnels, avec une finalité d'expérimentation, de formation et d'utilisation de modèles non restreints. Elle est principalement utilisée pour des besoins concrets à petite échelle, sans entraînement de modèles, mais en exploitant des modèles préexistants. Les coûts initiaux se situent entre 500 et 1000 euros, correspondant à l'achat d'un ordinateur portable ou d'une carte graphique gaming, tandis que les coûts opérationnels restent faibles, variant entre 0 et 100 euros par mois, principalement liés à la consommation électrique.

L'installation est optimisée pour une utilisation locale, avec une mise hors tension nocturne pour réduire les coûts énergétiques. Elle est gérée par Christian Lemoussel, qui l'utilise pour des projets personnels et professionnels, notamment via le site zonetuto.fr. La configuration permet une exécution rapide des tâches, potentiellement plus efficace que l'utilisation d'une API externe, tout en offrant une flexibilité et un contrôle total sur les modèles et les données traités."
"2025/09/18 2:14:49 PM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","qwen:3b","","Non, j'utilise des modèles existants","Openweb UI","Aucune - je n'utilise que le CPU","Rizen 8845HS","","","16Go Ram","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","100-500€ - équivalent d'une carte graphique simple","0 - pas de maintenance particulière","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Pour des tâches mineures ou un usage qui reste personnel","","","","","2025-09-18-11-14-49-000","Installation LLM Qwen-3B : partage et communauté en local","Découvrez l'installation locale de Qwen-3B sur CPU Ryzen 8845HS avec OpenWeb UI. Solution économique (100-500€) pour une IA performante sans GPU.","Optimisation d'un LLM Qwen-3B sur Serveur Local : Performance et Autonomie","Cette installation d'un modèle de langage local (LLM) est déployée sur un serveur hébergé localement, avec une configuration matérielle dédiée. L'utilisateur a opté pour une solution sur site ou via un prestataire IT local, en payant lui-même le matériel. Le système repose sur un processeur Rizen 8845HS, accompagné de 16 Go de RAM, sans carte graphique (GPU). Le modèle principal utilisé est Qwen-3B, avec une interface Openweb UI pour l'interaction. L'installation ne nécessite pas d'entraînement de modèles, l'utilisateur se limitant à des modèles préexistants.

L'installation est conçue pour un usage personnel exclusif, avec une performance évaluée comme rapide (plus de 40 tokens par seconde), offrant un temps de réponse en temps réel. Le coût initial se situe entre 100 et 500 euros, équivalent à celui d'une carte graphique simple, tandis que les coûts d'exploitation sont nuls, sans maintenance particulière. L'utilisateur a choisi cette solution pour des raisons de souveraineté des données, de confidentialité et d'expérimentation personnelle, sans partage de données personnelles ou professionnelles.

L'installation sert principalement à des tâches mineures ou à un usage personnel, sans finalité professionnelle. Les motivations incluent l'apprentissage, la maîtrise des technologies locales et la garantie d'un contrôle total sur les données traitées. Aucune information supplémentaire n'est fournie concernant la configuration réseau, le rack ou les détails techniques complémentaires. L'installation a été réalisée le 18 septembre 2025 à 11h14 UTC+3."
"2025/09/19 9:24:12 AM UTC+3","Oui","Directement sur ma machine","","","","mistral 7B","","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Dell Inspirons ","NVIDIA 4060 8Go","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0 - pas de maintenance particulière","","Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données","Pour des tâches mineures ou un usage qui reste personnel","","","","Behel","2025-09-19-06-24-12-000","Installation LLM Mistral 7B : partage et utilisation maison","Découvrez Mistral 7B installé sur votre Dell Inspiron via Ollama, gratuitement et sans frais supplémentaires. Une solution open source performante pour une IA l","Découvrez Mistral 7B : L'IA Puissante Directement sur Votre Machine","Cette installation d'un modèle de langage local (LLM) a été réalisée directement sur une machine personnelle, sans recours à un fournisseur externe ou à un service cloud. L'utilisateur a opté pour le modèle Mistral 7B, déployé via le logiciel Ollama, et n'utilise pas de modèles supplémentaires ni de capacités d'entraînement. La configuration matérielle inclut une carte graphique NVIDIA RTX 4060 avec 8 Go de mémoire, intégrée à un ordinateur portable Dell Inspiron, permettant des performances rapides, avec un débit de plus de 40 tokens par seconde, offrant une expérience de réponse en temps réel.

L'installation a été motivée par des préoccupations de confidentialité et de souveraineté des données. L'utilisateur manipule des informations sensibles et souhaite éviter tout partage de données personnelles ou professionnelles, même non confidentielles, avec des tiers. Aucun coût supplémentaire n'a été engagé, l'installation reposant uniquement sur des outils open source, sans frais de maintenance ou d'exploitation.

L'utilisation principale de cette installation est personnelle, pour des tâches mineures ou des besoins individuels. L'utilisateur n'envisage pas un usage professionnel intensif, mais privilégie une solution locale pour garantir un contrôle total sur ses données et ses interactions avec le modèle. Aucune information supplémentaire n'est fournie sur les détails techniques ou les performances spécifiques."
"2025/09/19 4:49:43 PM UTC+3","Oui","Directement sur ma machine","","","","mistral-small","mistral-nemo, gpt-oss, phi4","Non, j'utilise des modèles existants","Ollama","Oui sur machine personnelle","Zotac Magnus (Mini PC Monté maison)","RTX4060Ti 16GoVRAM","","","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Pour garantir la souveraineté de mes données","Oui elle répond à des besoins professionnels concrets à mon échelle","","","","","2025-09-19-13-49-43-000","Installation LLM Mistral Small pour usage communautaire en l","Découvrez Mistral-small installé sur un Zotac Magnus, un mini-PC puissant et personnalisé. Avec Ollama, profitez d'un LLM performant à domicile, pour 1000-3000€","Découvrez Mistral-Small : LLM Puissant et Léger pour une IA Locale et Performant","Cette installation LLM est configurée sur une machine personnelle, un Mini PC Zotac Magnus assemblé par l'utilisateur, équipé d'une carte graphique RTX 4060 Ti avec 16 Go de VRAM. L'installation utilise le logiciel Ollama et intègre plusieurs modèles de langage, dont Mistral Small comme modèle principal, ainsi que Mistral Nemo, GPT-OSS et Phi4 comme options secondaires. L'utilisateur n'entraîne pas de modèles mais exploite des modèles préexistants, ce qui simplifie la maintenance et réduit les coûts opérationnels.

L'installation est destinée à un usage personnel et professionnel limité, avec une capacité d'utilisation pour 1 à 5 personnes. Les performances sont estimées comme moyennes, avec un débit inférieur à 40 tokens par seconde, ce qui impose une attente pour les réponses ou un fonctionnement en arrière-plan. Le coût initial se situe entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, tandis que les coûts de maintenance sont nuls, grâce à l'absence de dépendance à un fournisseur externe ou à un service cloud.

L'installation répond à des besoins spécifiques liés à la manipulation de données confidentielles et à la souveraineté des données, justifiant ainsi le choix d'une solution locale plutôt que cloud. Elle sert également à des fins d'expérimentation et de formation, tout en répondant à des besoins professionnels concrets à petite échelle. L'absence de détails supplémentaires ou de documentation publique indique une configuration personnelle et non commerciale."
"2025/09/19 6:45:06 PM UTC+3","Oui","Directement sur ma machine","","","","gpt-oss","llama, deepseekr1","Non, j'utilise des modèles existants","Ollama, Open-webui, continue","Oui sur machine personnelle","ASUS(mobo)+MSI(gpu)+AMD(cpu)","NVIDIA RTX 4090","","","Juste pour moi","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","1000-3000€ - équivalent d'un ordinateur portable onéreux avec une pomme dessus","0 - pas de maintenance particulière","","Pour bidouiller et me former;Car je manipule des données confidentielles;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour garantir la souveraineté de mes données;Parce que je le peux","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-19-15-45-06-000","Installation maison de GPT-OSS : LLM communautaire pour usag","Installez GPT-OSS sur votre PC avec ASUS, MSI et AMD pour une IA locale puissante. Ollama + Open-webui pour une expérience fluide. Investissement : 1000-3000€, ","Découvrez gpt-oss : L'IA puissante et accessible directement sur votre machine","Cette installation d'un modèle de langage local (LLM) a été réalisée directement sur une machine personnelle, sans recours à un fournisseur externe ou cloud. L'utilisateur a opté pour une configuration autonome, privilégiant la confidentialité et la souveraineté des données. Le système repose sur des modèles pré-entraînés, notamment GPT-OSS, Llama et DeepSeekR1, sans formation personnalisée. Les logiciels utilisés incluent Ollama, Open-webui et Continue, permettant une interaction fluide avec les modèles.

L'installation s'appuie sur un matériel performant, incluant une carte graphique NVIDIA RTX 4090, ce qui garantit des vitesses de traitement élevées, dépassant 40 tokens par seconde, offrant ainsi une expérience en temps réel. La configuration matérielle comprend également une carte mère ASUS, une carte graphique MSI et un processeur AMD. L'investissement initial pour cette installation s'élève entre 1000 et 3000 euros, équivalent à un ordinateur portable haut de gamme, avec des coûts opérationnels nuls, grâce à l'absence de maintenance spécifique.

L'utilisateur a justifié cette installation par plusieurs motivations, notamment la manipulation de données confidentielles, le souhait de ne pas partager des informations personnelles ou professionnelles, et la volonté de garantir la maîtrise totale de ses données. Bien que l'installation ne soit pas utilisée à des fins professionnelles fonctionnelles, elle sert principalement à des expérimentations et à la formation. L'absence de coûts récurrents et la performance du système en font une solution viable pour un usage personnel et sécurisé."
"2025/09/19 8:11:16 PM UTC+3","Oui","Sur un cloud (je ne gère pas le matériel ou je le loue)","","deepinfra","Hébergement de l'inférence via des entry point type openai","gpt-oss-120b","Qwen3-coder","Non, j'utilise des modèles existants","Openwebui, sst/opencode ","Aucune car je passe par un hébergement cloud","","","","","1-5 personnes","Rapide (plus de 40 tokens/s) - les réponses ont un ressenti temps réel","0 - juste l'installation d'outils open source, pas d'achat supplémentaire","0-100€ / mois","","Pour bidouiller et me former;Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Parce que je le peux","Oui elle répond à des besoins professionnels concrets à mon échelle","","","Charles Bonnissent ","","charles-bonnissent-2025-09-19-17-11-16-000","Installation LLM gpt-oss-120b en cloud pour 5 utilisateurs -","Découvrez une installation LLM open source sur cloud avec gpt-oss-120b, OpenWebUI et OpenCode. Zéro coût, zéro matériel, juste des outils libres pour une IA per","Déploiement Cloud du Modèle gpt-oss-120b par Charles Bonnissent","Cette installation LLM est hébergée sur le cloud via le fournisseur DeepInfra, qui propose des endpoints d'inférence compatibles avec l'API OpenAI. L'utilisateur a opté pour une solution sans gestion matérielle directe, louant les ressources nécessaires. Le modèle principal utilisé est gpt-oss-120b, complété par Qwen3-coder pour des tâches spécifiques. L'installation repose sur des outils open source, notamment Openwebui et sst/opencode, sans entraînement de modèles personnalisés. Aucune carte GPU n'est déployée, l'infrastructure étant entièrement externalisée.

L'installation est conçue pour un usage limité à cinq personnes, avec une performance évaluée comme rapide, dépassant 40 tokens par seconde, offrant un rendu temps réel. Les coûts d'installation sont nuls, reposant uniquement sur des logiciels open source, tandis que les frais d'exploitation varient entre 0 et 100 euros par mois. L'utilisateur justifie son choix par des raisons personnelles, notamment l'absence de partage de données sensibles, ainsi que par la possibilité technique de déployer une telle solution.

Cette configuration répond à des besoins professionnels concrets, bien que limités en échelle. L'utilisateur, identifié sous le nom de Charles Bonnissent, a mis en place cette installation principalement pour des raisons d'apprentissage et d'expérimentation. Aucune information supplémentaire n'est fournie concernant les détails techniques ou les performances spécifiques des modèles utilisés."
"2025/09/21 12:42:12 AM UTC+3","Oui","Sur un serveur hébergé localement (je paie pour le matériel avec une installation sur site ou chez un prestataire IT local)","","","","llama2-uncensored","mistral, llama3","Non, j'utilise des modèles existants","ollama open webui","Oui sur un rack (installation sur site)","MS-01","RTX 3050 6Go","","70W","1-5 personnes","Moyen (moins de 40 tokens/s) - on peut attendre la réponse du modèle ou le laisser tourner","500-1000€ - équivalent d'un ordinateur portable simple ou d'une carte graphique de gaming","0-100€ / mois","","Je préfère ne pas partager mes données personnelles ou d'entreprise même non confidentielles;Pour utiliser des modèles débridés","Non elle n'est pas vraiment fonctionnelle","","","","","2025-09-20-21-42-12-000","Installation LLM Llama2-uncensored : partage et communauté e","Déployez Llama2-uncensored sur un serveur local (rack MS-01) avec Ollama et Open WebUI. Solution économique (500-1000€) pour une IA puissante et autonome, héber","Déploiement Local de Llama2-uncensored : Performance et Liberté sur Serveur Priv","Cette installation d'un modèle de langage local (LLM) est déployée sur un serveur hébergé localement, avec un matériel dédié payé par l'utilisateur. L'installation est réalisée sur site ou chez un prestataire IT local, sans recours à un fournisseur externe ou à un service cloud. Le système utilise principalement le modèle Llama2-uncensored, avec des options supplémentaires comme Mistral et Llama3, mais ne permet pas l'entraînement de modèles, se limitant à l'utilisation de modèles préexistants.

L'infrastructure repose sur un serveur identifié comme MS-01, équipé d'une carte graphique RTX 3050 de 6 Go, installée dans un rack. La consommation énergétique est estimée à 70W, et les performances sont qualifiées de moyennes, avec un débit inférieur à 40 tokens par seconde. L'installation est conçue pour un usage limité à 1 à 5 personnes, avec des coûts initiaux estimés entre 500 et 1000 euros, équivalents à ceux d'un ordinateur portable ou d'une carte graphique gaming. Les coûts opérationnels mensuels sont faibles, entre 0 et 100 euros.

L'installation a été motivée par la volonté de ne pas partager de données personnelles ou professionnelles, même non confidentielles, ainsi que par l'utilisation de modèles non censurés. Bien que l'installation soit fonctionnelle, elle n'est pas optimisée pour un usage professionnel. Le matériel et les performances actuels ne permettent pas une utilisation intensive ou rapide, mais suffisent pour des interactions basiques avec les modèles de langage."